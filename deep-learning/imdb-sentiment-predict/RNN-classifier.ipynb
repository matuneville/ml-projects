{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IMDB reviews sentiment analysis with RNN",
   "id": "fc83fafbdb57041f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data processing for RNN input",
   "id": "7c66609eb5f6a22b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Load data and create datasets for processing",
   "id": "c3169237772f1c6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T14:38:57.636873Z",
     "start_time": "2024-10-01T14:38:56.528695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_ds = IMDB(split='train')\n",
    "test_ds = IMDB(split='test')\n",
    "\n",
    "train_ds = list(train_ds)\n",
    "test_ds = list(test_ds)"
   ],
   "id": "bda29e61e5fe4798",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T14:38:58.336657Z",
     "start_time": "2024-10-01T14:38:58.322532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(13)\n",
    "train_ds, valid_ds = random_split(train_ds, [0.8, 0.2])\n",
    "print(f'Train dataset size: {len(train_ds)}, Valid dataset size: {len(valid_ds)}')"
   ],
   "id": "9c104d09f9d9080",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 20000, Valid dataset size: 5000\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T16:26:41.569129Z",
     "start_time": "2024-10-01T16:26:41.564634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's see how it looks\n",
    "train_ds[2002]"
   ],
   "id": "e4a2c18b1be3da59",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " 'Dick Clement and Ian La Frenais have a solid hit rate as far as their TV work is concerned. However, their film work has been much more chequered (2008\\'s The Bank Job was fine, the previous year\\'s Across The Universe decidedly weak, for instance).<br /><br />Still Crazy, fortunately, is a solid success. It has a great story, excellent performances, a lot of humour, fabulous music and, above everything else, real heart.<br /><br />I savour \"moments\", and this film has one of them - just when everything is going pear-shaped at the festival reunion performance...<br /><br />Hugely enjoyable.')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Now we find unique words using text preprocessor from previous project",
   "id": "f4827a4abf746970"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T14:54:44.775852Z",
     "start_time": "2024-10-01T14:54:41.663794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "# Get text words, ignore html tags, add emojis at the end\n",
    "def tokenizer(text):\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    emots = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub(r'[\\W]+', ' ', text.lower()) + ' '.join(emots).replace('-', ''))\n",
    "    tokenized_text = text.split()\n",
    "    return tokenized_text\n",
    "\n",
    "# Hashmap of <word, counts>\n",
    "token_counts = Counter()\n",
    "\n",
    "for label, review in train_ds:\n",
    "    token_counts.update(tokenizer(review))\n",
    "    \n",
    "print(f'Vocab size: {len(token_counts)}')"
   ],
   "id": "19a4f63fa8e1e00d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 69161\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. We will encode unique tokens into numbers  \n",
    "\n",
    "For that, we use the `Vocab` class form torchtext that allow us to create such a mapping and encode the entire dataset.\n",
    "\n",
    "The point of the having the ordered words by occurrences is to ensure that the tokens are indexed based on their frequency of occurrence, with more common tokens getting lower integer values, which is a typical practice in NLP to optimize learning."
   ],
   "id": "b49fd487fb4106b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T16:15:44.130553Z",
     "start_time": "2024-10-01T16:15:43.933196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sorted_token_counts = sorted(token_counts.items(), key = lambda item: item[1], reverse=True)\n",
    "ordered_token_map = OrderedDict(sorted_token_counts)\n",
    "\n",
    "from torchtext import vocab\n",
    "\n",
    "vocab = vocab.vocab(ordered_token_map)\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1) # index 1 is the placeholder for unknown tokens (not the case of all the words from IMDB reviews)\n",
    "vocab.set_default_index(1)"
   ],
   "id": "99a10bc13afb5173",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T16:25:40.736128Z",
     "start_time": "2024-10-01T16:25:40.732041Z"
    }
   },
   "cell_type": "code",
   "source": "print([vocab[token] for token in tokenizer('The film was not SO good :( but...')])",
   "id": "50ab53ef305819f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 20, 14, 24, 37, 50, 19, 10088]\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define functions for transformations",
   "id": "a47a5f3adf83fc43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T17:50:22.741529Z",
     "start_time": "2024-10-01T17:50:22.736618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def text_pipeline(text):\n",
    "    return [vocab[token] for token in tokenizer(text)]\n",
    "\n",
    "def label_pipeline(label):\n",
    "    # label 2 is positive review, 1 is negative\n",
    "    return 1. if label == 2 else 0."
   ],
   "id": "85f8b7390a3fa047",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Put together the processed labels and texts",
   "id": "80c34e2985ca6fa1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T17:50:24.184513Z",
     "start_time": "2024-10-01T17:50:24.179064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_batch_fn(batch):\n",
    "    labels, texts, lengths = [], [], []\n",
    "    for label, text in batch:\n",
    "        labels.append(label_pipeline(label))\n",
    "        tokens_vocab = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        texts.append(tokens_vocab)\n",
    "        lengths.append(tokens_vocab.size(0))\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    # pad consecutive elements that are to be combined into a batch with placeholder values (0s)\n",
    "    # so that all sequences within a batch have the same shape\n",
    "    # this method pads every tensor as much as necessary to match the max size\n",
    "    padded_texts = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True)\n",
    "    \n",
    "    return padded_texts, labels, lengths"
   ],
   "id": "ec70616df6b94672",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's see how this works",
   "id": "c814c7f889676803"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T17:50:25.655786Z",
     "start_time": "2024-10-01T17:50:25.640844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "example_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_batch_fn)\n",
    "\n",
    "padded_texts, labels, lengths = next(iter(example_dl))\n",
    "print('Labels: ', labels)\n",
    "print('Lengths (not padded): ', lengths)\n",
    "print('Shapes (padded): ', padded_texts.shape)\n",
    "print('Textx (padded): ', padded_texts)"
   ],
   "id": "c071fe298758c562",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  tensor([0, 0, 0, 0])\n",
      "Lengths (not padded):  tensor([156, 210, 187, 136])\n",
      "Shapes (padded):  torch.Size([4, 210])\n",
      "Textx (padded):  tensor([[ 1313,  4623,    10,   115,  1659,  4413,    10,    65,    81,     3,\n",
      "            25,   125,    27,   114,    17,    35,  1284,   212,    10,    51,\n",
      "            66,    89,    11,     7,     4,    18,    12,   235,   166,    50,\n",
      "            15,     4,   226,    19,     8,    13,  3904,  2354,     3,  1145,\n",
      "           704,    10,   215,     2,  1172,   341,   288,   571,     4,  3918,\n",
      "           243, 12870,  4241,   502,   253,    17,    35, 14472,   171,     3,\n",
      "            37,   125,  5372,  6442,     2,  1229,   618,   399,    10,    51,\n",
      "            21,   373,     2,    30,    12,    13,    24,  1389,  5266,     7,\n",
      "             4,   192, 12146,   120,   275,    66,     8,    47,    22,   149,\n",
      "            24,  1432,   197,     6, 18618,   202,     4,    18,   110,    37,\n",
      "          1064,    22,    51,    21,   795,     8,     3,    37,  1481,    22,\n",
      "            51,    21,   385,     8,    66,     8,    47,    22,    39,   932,\n",
      "           365,   158,    12,   167,    22, 42517,    66,     8,    47,    22,\n",
      "           115,     6,   104,  1659,  4413,   158,    17,    35, 35244,   226,\n",
      "            66,     8,    47,     8,   265,    23,   523,    32,   313,    16,\n",
      "           861,   873,   832,     2,   682,  3160,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   10,   248,     4,   671,   490,  7544,   335,    37,  2824,     6,\n",
      "           133,    10,  1230,    11,    18,   206,    57,  2976,     8,    42,\n",
      "           229,    11,    18,    14,    37,   514,    10,   164,    21,    57,\n",
      "           193,     8,   144,     6,     2,  1071,    10,   577,    21,   183,\n",
      "           252,   329,     6,    29,  5240,     6,    11,   396,  5059,    10,\n",
      "            43,  4163,     8,     9,     2,  1158,   112,   331,     2,   278,\n",
      "             8,    14,   293,     2,  1863,     6,    29,   493,     6,  1361,\n",
      "             8,   243,     2,   116,   291,    21,    12,    75,     8,   291,\n",
      "            21,    50,    42,   229,     2,    64,    14,   514,     3,     2,\n",
      "           288,    14,   143,    10,  7710,    25,    14,     4,  1906,   126,\n",
      "          4731,    27,   111,    14,     4,   778,     5,  2507,    10,   199,\n",
      "            27,  1719, 14919,    14,    35,   369,  2742,     6,   121,    27,\n",
      "          1655,  3043,     4,   878,    61,    28,    77,    74,    74,   127,\n",
      "            10,    82,   770,  2256,    37,    11,   288,   206,     4,   824,\n",
      "         16631,    70,  1820, 21662,    31,    10,   770,    12,    10,   122,\n",
      "            76,    43,    39,    11,    12,    26,   671,  4667,     3, 14947,\n",
      "             3,    58,    79,   121,    12,   501,   152,    21, 10472,    31,\n",
      "            42,   229,     8,    14,     4,   514,    18,    47,  1171,    28,\n",
      "             2,  5671,     6,    66,     8,  1365,    42,   127,   241,   782,\n",
      "             8,    42,   832,     8,     3,  1361,     8,     9,     2,  1227,\n",
      "             3,   564,     2,  3916,  6125,    23,     8,     9,     2,  1991],\n",
      "        [   10,   248,     4,   192,   335,     5,  3042, 23648,   412,   321,\n",
      "            88,    15,     2,   326,     5,     4,  1126,    18,    24,    43,\n",
      "             4,   220,   171,    91,    70,   658,   469,    10,   550,    20,\n",
      "          4747,     9,  1149,     3,     2,    62,   150,    10,    51,   133,\n",
      "            12,    10,   164,    21,    39,    14,    12,     2,    20,    14,\n",
      "            91,     9,    35,  2669,   171,     5,   498,     3,    40,    14,\n",
      "            59,   959,  6048,    42, 60222, 18800,    10,    28,     6,   133,\n",
      "            12,     2,   116,   902,   143,     6,    29,  4716,    19,  3042,\n",
      "             7,    35,   319,   275,    10,    28,     6,  9624,     8,    56,\n",
      "             6,  2290,   459,     3,     2,   680,   178,   902,   143,     6,\n",
      "            29,  4716,    10,   115,  3042, 23648,     3,    25,    91,     2,\n",
      "            20, 22117,   873,    10,    61,    28,   394,     8,   123,    15,\n",
      "           162,  2290,    20,    10,   255,     2,  1551,   134,  1852,     3,\n",
      "          1768,    19,     2,   154,    12,   186,   243,    32,     2,   131,\n",
      "            71,   184,    50,    19,     2,   465,   299,     5,     2,  1269,\n",
      "            45,    71,   184,    75,    40,    26,    49, 16306,     9,     2,\n",
      "            20,  2394,    19,    33,    26, 60223,  1089,     2,   363,  4730,\n",
      "             5,     2,    20,    31,     9,    31,    10,   495,     2,    20,\n",
      "            85,  3042, 23648,    14,     9,     8,  2052,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   10,   248,    38,     2,  7812, 11938,   293,  1597,     3,  1394,\n",
      "             9, 14364,    16,     4,   168,   155,    11,    18,    14,    96,\n",
      "           123,    15,   227,    15,   232,     8,   166,    39, 14364,    10,\n",
      "           215,  4025,     9,     2,   959,     5,    30,   134,  2278,   152,\n",
      "            21,    28,  4025,    10,   484,    12,   562,    52,     4,    18,\n",
      "            12,     7,   427,     6,    29,     9,  2278,     7,   788,     9,\n",
      "          3226,     2,  2364,    26,    82,    65,    75,    33,   141,    28,\n",
      "          1791,   154,    38,  2278,     6,   298,     2,   529,    40,     4,\n",
      "           176,     5,  5660,   154,    38,  2278,    45,     9,   336,     2,\n",
      "            18,     7,    65,   603,   151,    85,     8,     7,     4,   283,\n",
      "            64,    10,  6092,    12,     2,   444,     7,   255,     3,  7183,\n",
      "             2,    30,    50,   150,     7,    12, 13476,     5,    41,   341,\n",
      "            69,   148,    28,     2, 11610,  4623,     6,   346,   170,   998,\n",
      "           432,   961,    99,    33,    26, 10079,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Create DataLoaders for mini-batches",
   "id": "16e4de802dc3effb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_size = 16\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch_fn)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch_fn)"
   ],
   "id": "829c12f3731e4762"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Building RNN",
   "id": "f0813efc8f68ec40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, output_size=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        "
   ],
   "id": "f1fbf490f9bf3159"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
