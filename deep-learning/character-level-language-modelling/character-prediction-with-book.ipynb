{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Character-level Language Modeling\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project focuses on training a character-based recurrent neural network (RNN) model to generate text based on the works of H.P. Lovecraft. The choice of Lovecraft's writing serves as a rich source of vocabulary and narrative complexity, providing an intriguing dataset for exploring the capabilities of RNNs in text generation.  \n",
    "\n",
    "By using character-level modeling, the aim is to capture the nuances of language structure without explicitly attempting to replicate any particular writing style. The model focuses solely on predicting individual characters based on prior context, rather than understanding the broader semantics or thematic elements of the text. The project highlights the process of data preparation, model architecture design, and evaluation of generated outputs, ultimately shedding light on the strengths and limitations of character-based text generation."
   ],
   "id": "db85f47d6721e793"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Text preprocessing",
   "id": "f9fe855cdef9a9f3"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:58.161775Z",
     "start_time": "2024-10-09T15:27:58.157784Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "import subprocess\n",
    "\n",
    "# install At the Mountains of Madness - H.P. Lovecraft\n",
    "command = ['curl', '-O', 'https://www.gutenberg.org/files/70652/70652-0.txt']\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "    print(\"File downloaded successfully.\")\n",
    "except subprocess.CalledProcessError as error:\n",
    "    print(f'Error occurred: {error.stderr}')\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport subprocess\\n\\n# install At the Mountains of Madness - H.P. Lovecraft\\ncommand = [\\'curl\\', \\'-O\\', \\'https://www.gutenberg.org/files/70652/70652-0.txt\\']\\n\\ntry:\\n    result = subprocess.run(command, capture_output=True, text=True, check=True)\\n    print(\"File downloaded successfully.\")\\nexcept subprocess.CalledProcessError as error:\\n    print(f\\'Error occurred: {error.stderr}\\')\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:58.319060Z",
     "start_time": "2024-10-09T15:27:58.277644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('70652-0.txt', mode='r', encoding='utf8') as file:\n",
    "    book = file.read()"
   ],
   "id": "2b95df82808b2d7a",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:58.467593Z",
     "start_time": "2024-10-09T15:27:58.354713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove first and last pages by Project Guttenberg\n",
    "begin_index = book.find('At the MOUNTAINS of MADNESS')\n",
    "end_index = book.find('THE END')\n",
    "\n",
    "book = book[begin_index:end_index]"
   ],
   "id": "c0b9e4eb7c12d851",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:58.642911Z",
     "start_time": "2024-10-09T15:27:58.493289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique_chars = set(book)\n",
    "print(sorted(unique_chars))"
   ],
   "id": "56c009e3929b15c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '°', '´', '×', 'æ', 'é', 'ë', 'ï', 'ö']\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we will remove some characters that do not give further information for our task:",
   "id": "6a527a850421db6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:58.824632Z",
     "start_time": "2024-10-09T15:27:58.666464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# remove not important characters\n",
    "book = book.replace('\\n', ' ')\n",
    "book = book.replace('--', ',')\n",
    "book = book.replace('----', ' ')\n",
    "for char in ['*', '[', ']', '×', '_']:\n",
    "    book = book.replace(char, '')\n",
    "\n",
    "# replace multiple spaces/newlines with a single space\n",
    "book = re.sub(r'\\s+', ' ', book)\n",
    "# fix commas after multiple spaces removal\n",
    "book = re.sub(r'(\\w),\\s*(\\w)', r'\\1, \\2', book)\n",
    "\n",
    "unique_chars = set(book)"
   ],
   "id": "c7c5560c5ecf26b8",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:58.968792Z",
     "start_time": "2024-10-09T15:27:58.843456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Length of book in characters: ', len(book))\n",
    "print('Unique characters in book: ', len(unique_chars))"
   ],
   "id": "26ac86139428915e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of book in characters:  241251\n",
      "Unique characters in book:  80\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will need the chars as integers for numeric representation. A NumPy array will be better for fast processing:",
   "id": "1d5ed2dbf9bf9db2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:59.148941Z",
     "start_time": "2024-10-09T15:27:59.000608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "sorted_chars = np.array(sorted(unique_chars))\n",
    "char2int = {char : integer for integer, char in enumerate(sorted_chars)}\n",
    "encoded_book = np.array([char2int[char] for char in book], dtype=np.int32)"
   ],
   "id": "faf25982337f0570",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check the encoded book size is the same as before:",
   "id": "e0f965f641d870c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:59.275220Z",
     "start_time": "2024-10-09T15:27:59.173642Z"
    }
   },
   "cell_type": "code",
   "source": "print(encoded_book.shape)",
   "id": "24e5af6be3836eb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(241251,)\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now print some encoded line:",
   "id": "1be840856104027"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:59.424080Z",
     "start_time": "2024-10-09T15:27:59.327862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first_line = slice(248, 348)\n",
    "print('Original text:\\n\"' + book[first_line] + '\"\\n')\n",
    "print('Encoded text:\\n', encoded_book[first_line])\n",
    "print('\\nLengths of both texts: ', len(encoded_book[first_line]), len(book[first_line]))"
   ],
   "id": "c0240fd534c30ae0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "\" I am forced into speech because men of science have refused to follow my advice without knowing why\"\n",
      "\n",
      "Encoded text:\n",
      " [ 0 30  0 47 59  0 52 61 64 49 51 50  0 55 60 66 61  0 65 62 51 51 49 54\n",
      "  0 48 51 49 47 67 65 51  0 59 51 60  0 61 52  0 65 49 55 51 60 49 51  0\n",
      " 54 47 68 51  0 64 51 52 67 65 51 50  0 66 61  0 52 61 58 58 61 69  0 59\n",
      " 71  0 47 50 68 55 49 51  0 69 55 66 54 61 67 66  0 57 60 61 69 55 60 53\n",
      "  0 69 54 71]\n",
      "\n",
      "Lengths of both texts:  100 100\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And in reverse:",
   "id": "def30ca3cbd25f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:59.661379Z",
     "start_time": "2024-10-09T15:27:59.518262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Encoded text:\\n', encoded_book[first_line], \"\\n\")\n",
    "print('Decoded text:', ''.join(sorted_chars[encoded_book[first_line]]))"
   ],
   "id": "4d49f95f29e6322f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text:\n",
      " [ 0 30  0 47 59  0 52 61 64 49 51 50  0 55 60 66 61  0 65 62 51 51 49 54\n",
      "  0 48 51 49 47 67 65 51  0 59 51 60  0 61 52  0 65 49 55 51 60 49 51  0\n",
      " 54 47 68 51  0 64 51 52 67 65 51 50  0 66 61  0 52 61 58 58 61 69  0 59\n",
      " 71  0 47 50 68 55 49 51  0 69 55 66 54 61 67 66  0 57 60 61 69 55 60 53\n",
      "  0 69 54 71] \n",
      "\n",
      "Decoded text:  I am forced into speech because men of science have refused to follow my advice without knowing why\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modelling the RNN\n",
    "\n",
    "We will construct our model for character-based language modeling. The custom dataset class takes encoded chunks of text as input. Each chunk will be composed of a sequence of characters, where the model will predict the next character given the current sequence.\n"
   ],
   "id": "cc7e148558904a8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:59.894186Z",
     "start_time": "2024-10-09T15:27:59.688685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "seq_length = 50\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "encoded_chunks = [encoded_book[i : i+chunk_size] for i in range(0, len(encoded_book) - chunk_size)]"
   ],
   "id": "6f6ac97ce95d8f9c",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:27:59.929650Z",
     "start_time": "2024-10-09T15:27:59.912905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encoded_chunks):\n",
    "        super().__init__()\n",
    "        self.encoded_chunks = encoded_chunks\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.encoded_chunks[idx][:-1]\n",
    "        y = self.encoded_chunks[idx][1:]\n",
    "        return x, y"
   ],
   "id": "63acb12eb69884a8",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:28:00.077041Z",
     "start_time": "2024-10-09T15:27:59.954618Z"
    }
   },
   "cell_type": "code",
   "source": "text_dataset = TextDataset(encoded_chunks)",
   "id": "70330d50d9d8d4e0",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Every next character of each character is the one to be predicted. Thus, for a random chunk, input and target will look like:",
   "id": "add66bbb4a528537"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:28:00.225490Z",
     "start_time": "2024-10-09T15:28:00.096607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for _ in range(3):\n",
    "    x, y = text_dataset[np.random.randint(0, len(text_dataset))]\n",
    "    print('Input: ', ''.join(sorted_chars[x]))\n",
    "    print('Target:', ''.join(sorted_chars[y]), '\\n')"
   ],
   "id": "f687efe506c9a6cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  d be called decadent in comparison with that of sp\n",
      "Target:  be called decadent in comparison with that of spe \n",
      "\n",
      "Input:  diate, but was clearly something more. It was part\n",
      "Target: iate, but was clearly something more. It was partl \n",
      "\n",
      "Input:  al recession toward the antarctic became very plai\n",
      "Target: l recession toward the antarctic became very plain \n",
      "\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:28:00.410196Z",
     "start_time": "2024-10-09T15:28:00.249125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "text_dataloader = DataLoader(text_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "id": "6a07139d8650a348",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Our model comprises an **embedding layer** that transforms character indices into dense vectors, followed by a **bidirectional LSTM layer** that captures temporal dependencies in the sequence from both forward and backward directions. This bidirectional setup enhances the model's ability to understand context, as it considers both preceding and succeeding characters, leading to a richer representation of the input sequence. Finally, we have a fully connected layer that outputs the logits for each character in our vocabulary. This architecture not only improves the coherence and relevance of the generated text but also allows the model to generate more meaningful outputs by leveraging information from the entire sequence, thus enhancing its overall performance in character-based text generation tasks."
   ],
   "id": "aea206eac7bd72e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:28:00.682456Z",
     "start_time": "2024-10-09T15:28:00.432138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        # Store the hidden size, note that hidden size will be doubled for bidirectional\n",
    "        self.rnn_hidden_size = hidden_size\n",
    "        # LSTM with bidirectional=True\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        # Fully connected layer input size should be 2 * hidden_size due to bidirection\n",
    "        self.fc = nn.Linear(2 * hidden_size, vocab_size)  # Update input size to account for bidirectional\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x)  # shape = [batch_size, seq_length, embedding_size]\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))  # shape = [batch_size, seq_length, 2 * hidden_size]\n",
    "        # Reshape output for the fully connected layer\n",
    "        out = out.reshape(-1, 2 * self.rnn_hidden_size)  # Flatten to [batch_size * seq_length, 2 * hidden_size]\n",
    "        out = self.fc(out)  # Pass through fully connected layer\n",
    "        return out, hidden, cell  # Return the output and the hidden/cell states\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Create two tensors for hidden and cell states initialized to zero\n",
    "        # Shape: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        return (torch.zeros(2, batch_size, self.rnn_hidden_size).to(device),\n",
    "                torch.zeros(2, batch_size, self.rnn_hidden_size).to(device))\n"
   ],
   "id": "58ec13f6e35a43ad",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:28:00.854086Z",
     "start_time": "2024-10-09T15:28:00.703306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = len(unique_chars)\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = RNNModel(vocab_size, embedding_size, hidden_size)\n",
    "model"
   ],
   "id": "b8e531c5086bb691",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=1024, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "With our RNN model and DataLoader set up, we are ready to move on to training the model."
   ],
   "id": "c4a80c22cbc99c76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Training\n",
    "\n",
    "In this section, we will train our RNN model for character prediction over 15,000 epochs. During training, we will monitor the model's performance every 500 epochs by tracking both the loss and accuracy.  \n",
    "\n",
    "For this multiclass classification task, where the goal is to predict one of 78 characters, we utilize the Cross Entropy Loss function (since the outputs are logits), and an Adam optimizer.\n"
   ],
   "id": "f24a2b98d8697795"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:28:01.006350Z",
     "start_time": "2024-10-09T15:28:00.875501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "id": "4947fdef509f7d65",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=1024, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:57:02.242097Z",
     "start_time": "2024-10-09T15:28:01.051060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 15001\n",
    "torch.manual_seed(13)\n",
    "for epoch in range(num_epochs):\n",
    "    # initialize hidden and cell states and move them to the device\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    hidden, cell = hidden.to(device), cell.to(device)\n",
    "\n",
    "    seq_batch, target_batch = next(iter(text_dataloader))\n",
    "    seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()  # reset gradients\n",
    "    loss = 0\n",
    "    total_correct = 0  # keep track of correct predictions\n",
    "    total_count = 0    # keep track of total number of predictions\n",
    "    \n",
    "    for c in range(seq_length):\n",
    "        # reshape seq_batch[:, c] to [batch_size, 1]\n",
    "        input_tensor = seq_batch[:, c].unsqueeze(1)  # Shape: [batch_size, 1]\n",
    "        pred, hidden, cell = model(input_tensor.to(device), hidden, cell)  # Send to device\n",
    "        # Compute loss\n",
    "        loss += loss_fn(pred, target_batch[:, c].long())\n",
    "        # Calculate accuracy\n",
    "        predicted_chars = torch.argmax(pred, dim=1)  # Get the index of the max log-probability\n",
    "        correct_predictions = (predicted_chars == target_batch[:, c]).sum().item()\n",
    "        total_correct += correct_predictions\n",
    "        total_count += target_batch.size(0)  # Add the batch size to total_count\n",
    "    \n",
    "    loss.backward()  # backpropagate\n",
    "    optimizer.step()  # take step forward\n",
    "    \n",
    "    # compute average loss per sequence\n",
    "    avg_loss = loss.item() / seq_length\n",
    "    # compute accuracy\n",
    "    accuracy = total_correct / total_count\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch} loss: {avg_loss:.4f}, accuracy: {accuracy:.4f}')"
   ],
   "id": "f9d4c5c0d927b269",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.3896, accuracy: 0.0106\n",
      "Epoch 500 loss: 1.4301, accuracy: 0.5725\n",
      "Epoch 1000 loss: 1.2228, accuracy: 0.6088\n",
      "Epoch 1500 loss: 1.1140, accuracy: 0.6512\n",
      "Epoch 2000 loss: 0.9552, accuracy: 0.7156\n",
      "Epoch 2500 loss: 0.7970, accuracy: 0.7506\n",
      "Epoch 3000 loss: 0.7471, accuracy: 0.7738\n",
      "Epoch 3500 loss: 0.6518, accuracy: 0.8025\n",
      "Epoch 4000 loss: 0.5552, accuracy: 0.8375\n",
      "Epoch 4500 loss: 0.5396, accuracy: 0.8375\n",
      "Epoch 5000 loss: 0.4778, accuracy: 0.8550\n",
      "Epoch 5500 loss: 0.4763, accuracy: 0.8569\n",
      "Epoch 6000 loss: 0.4337, accuracy: 0.8662\n",
      "Epoch 6500 loss: 0.3822, accuracy: 0.8944\n",
      "Epoch 7000 loss: 0.4198, accuracy: 0.8706\n",
      "Epoch 7500 loss: 0.4207, accuracy: 0.8731\n",
      "Epoch 8000 loss: 0.3925, accuracy: 0.8881\n",
      "Epoch 8500 loss: 0.3609, accuracy: 0.8931\n",
      "Epoch 9000 loss: 0.3584, accuracy: 0.8994\n",
      "Epoch 9500 loss: 0.3433, accuracy: 0.8944\n",
      "Epoch 10000 loss: 0.3545, accuracy: 0.8988\n",
      "Epoch 10500 loss: 0.3501, accuracy: 0.8969\n",
      "Epoch 11000 loss: 0.3634, accuracy: 0.8956\n",
      "Epoch 11500 loss: 0.3467, accuracy: 0.9038\n",
      "Epoch 12000 loss: 0.3263, accuracy: 0.9025\n",
      "Epoch 12500 loss: 0.3217, accuracy: 0.8956\n",
      "Epoch 13000 loss: 0.3430, accuracy: 0.9038\n",
      "Epoch 13500 loss: 0.3321, accuracy: 0.8988\n",
      "Epoch 14000 loss: 0.3331, accuracy: 0.9100\n",
      "Epoch 14500 loss: 0.3260, accuracy: 0.9031\n",
      "Epoch 15000 loss: 0.3538, accuracy: 0.9031\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Model saving and loading (if necessary).",
   "id": "cb91cd1ee268f93d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:57:02.485590Z",
     "start_time": "2024-10-09T15:57:02.287072Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'model.pt')",
   "id": "7abd62d44e2beb08",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T15:57:02.826780Z",
     "start_time": "2024-10-09T15:57:02.588699Z"
    }
   },
   "cell_type": "code",
   "source": "# model.load_state_dict(torch.load('model.pt'))",
   "id": "9b58aaca5bcefc3a",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Results: generating new text\n",
    "\n",
    "In this section, we implement a text generation process using an **autoregressive model**, where each new character is generated based on the previously generated characters. The model starts with a given input string, which is encoded into a sequence of integers representing characters. This sequence is passed through our RNN model, which predicts the next character in the form of logits. Instead of always selecting the character with the highest probability (which would lead to repetitive output), we use a categorical distribution to randomly sample the next character based on the logits. This sampled character is then added to the generated text, and the process continues by feeding the updated sequence back into the model to generate the next character. This method, called autoregression, allows the model to generate diverse and coherent text one character at a time, where each new prediction depends on the context of the previously generated sequence."
   ],
   "id": "4a8bc9435a50c4b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:07:07.967841Z",
     "start_time": "2024-10-09T16:07:07.961199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def sample_text(model, start_str, length_to_generate, scale=1.0):\n",
    "    # lower scale = more randomness\n",
    "    # encode the starting string into integer tensor\n",
    "    encoded_input = torch.tensor([char2int[s] for s in start_str])\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))  # reshape to (1, sequence_length), like batch_size = 1\n",
    "    encoded_input = encoded_input.to(device)\n",
    "\n",
    "    generated_str = start_str  # initialize the generated string with the starting string\n",
    "\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)  # initialize hidden and cell states for batch_size = 1 sequence\n",
    "    hidden , cell = hidden.to(device), cell.to(device)\n",
    "\n",
    "    # pass through the model for the initial characters of the input string\n",
    "    for c in range(len(start_str) - 1):\n",
    "        _, hidden, cell = model(encoded_input[:, c].view(1, 1), hidden, cell)  # forward pass for each char\n",
    "\n",
    "    last_char = encoded_input[:, -1]  # get the last character from the input for prediction\n",
    "    for i in range(length_to_generate):\n",
    "        logits, hidden, cell = model(last_char.view(1, 1), hidden, cell)  # forward pass for the last char\n",
    "        logits = torch.squeeze(logits, 0)  # remove extra dimensions for logits\n",
    "        scaled_logits = logits * scale  # scale logits for randomness/creativity in sampling\n",
    "        m = Categorical(logits=scaled_logits)  # create a categorical distribution from logits\n",
    "        last_char = m.sample()  # sample the next character from the distribution\n",
    "        generated_str += str(sorted_chars[last_char])  # append the generated character to the output string\n",
    "        \n",
    "    return generated_str"
   ],
   "id": "3c32a98ceb1a28b4",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:08:10.254971Z",
     "start_time": "2024-10-09T16:08:09.664413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(13)\n",
    "result = sample_text(model, start_str='The mountains were', length_to_generate=500)\n",
    "print(result)"
   ],
   "id": "dcf96500151e305f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mountains were of all imaginable shapes and proportions, ranificial distinctness. As we looked more steady distant scenes can sometimes be reflected, refrained from sheer and scarred the height of the second in a shrieking subway train, a shapeless congeries of protoplasmic bubbles, faintly self-luminous coast line of Queen Mary and Knox Lands. Then, in about a quarter of a mile that nameless scent before the lowest as the wild tales of cosmic hill things from other tasks to work on them. It was after four mi\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:10:25.975890Z",
     "start_time": "2024-10-09T16:10:25.422724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1310)\n",
    "result = sample_text(model, start_str='The beings were ', length_to_generate=500, scale=4)\n",
    "print(result)"
   ],
   "id": "382a7487022b1ccb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The beings were unmistakable. In the building of land cities the huge stone blocks of the high towers were generally lifted by vast-winged pterodactyls of a species heretofore unknown to palæontology. The persistence with which the Old Ones survived various geologic changes and concocted earlier years. Danforth, indeed, is known to be among the first thing I remember of the rest of the journey was hearing him light-headedly chant a hysterical formula in which I alone of mankind could have found anything but ins\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:11:34.195801Z",
     "start_time": "2024-10-09T16:11:33.930086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(13)\n",
    "result = sample_text(model, start_str='our camp was', length_to_generate=250, scale=5)\n",
    "print(result)"
   ],
   "id": "66bfeb8e95f61d9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our camp was left, of what had disappeared, and of how the madness of a lone survivor might have conceived the inconceivable, a wild trip across the momentous divide and over the unsampled secrets of an elder and porternal terrace had once existed there. Under t\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:13:55.278632Z",
     "start_time": "2024-10-09T16:13:55.075512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1310)\n",
    "result = sample_text(model, start_str='The cult of Cth', length_to_generate=200, scale=7)\n",
    "print(result)"
   ],
   "id": "a5b0267dc2aa8f57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cult of Cthulhu, soon began filtering down from cosmic infinity and preparing to do some exploration on foot. Though the culture was mainly urban, some agriculture and much stock raising existed. Mining and a li\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:15:11.989457Z",
     "start_time": "2024-10-09T16:15:11.791440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(2002)\n",
    "result = sample_text(model, start_str=' was written in the Necr', length_to_generate=200, scale=5)\n",
    "print(result)"
   ],
   "id": "7ff6b5f2ee31814e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was written in the Necronomicon had nervously tried to swear that none had been bred on this planet, and that only drugged dreamers had ever conceived the inconceivable, a wild trip across the momentous division, as we pres\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T16:16:49.696925Z",
     "start_time": "2024-10-09T16:16:49.152310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(13)\n",
    "result = sample_text(model, start_str='The cities would ', length_to_generate=500, scale=3)\n",
    "print(result)"
   ],
   "id": "94684a00690f1887",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cities would have been half cleared, and the glacial surface from where the towers projected was strewn with fallen plateau and with our thickest furs. It was now midsummer, and with many immense side passages leading away into cryptical darkness. Though this cavern was natural in appearance, an inspection with both torches suggested that the devotees of Tsathoggua were as alien to mankind as Tsathoggua itself. Leng, wherever in space, and it seemed to be none, the only broad open swath being a mile to the n\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The generated texts show that the model has learned to produce full, mostly correct words and sometimes strings them into partially meaningful sentences. While the output captures elements of Lovecraft's vocabulary and tone, particularly his archaic and formal style, it falls short of achieving true coherence and narrative flow, as it is only a character-based language model. These models excel at learning short-term dependencies but struggle with maintaining consistent logic or thematic continuity over longer text sequences. Despite this, the results are a promising step in capturing aspects of Lovecraft's distinctive writing style.\n",
    "\n"
   ],
   "id": "497094debdb20b4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ab58753609baa86f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
