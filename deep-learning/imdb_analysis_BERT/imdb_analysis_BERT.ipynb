{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-tuning a BERT model for IMDb review classification",
   "id": "c7447cb073492d85"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-28T12:33:41.284516Z",
     "start_time": "2024-10-28T12:32:29.493938Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "\n",
    "# from Hugging Face\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data preparation\n",
    "### Loading Dataset\n",
    "\n",
    "Already downloaded (for previous Logistic Regression and RNN based models). "
   ],
   "id": "e50d7a96a0411e3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:34:48.940073Z",
     "start_time": "2024-10-28T12:34:46.478532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = \"../supervised-learning/imdb-review-classification/movie_data.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df = df.sample(frac=0.5, random_state=1).reset_index(drop=True)\n",
    "df.sample(5)"
   ],
   "id": "87652eae8dcc7929",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  review  sentiment\n",
       "13574  Because of the 1988 Writers Guild of America s...          0\n",
       "19604  Aside from a few good moments of fairly raw vi...          0\n",
       "22161  I should no longer be surprised when critics m...          1\n",
       "1703   Prepare to meet your Messiah - they call him M...          1\n",
       "376    Greetings again from the darkness. Based on th...          1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13574</th>\n",
       "      <td>Because of the 1988 Writers Guild of America s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19604</th>\n",
       "      <td>Aside from a few good moments of fairly raw vi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22161</th>\n",
       "      <td>I should no longer be surprised when critics m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>Prepare to meet your Messiah - they call him M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>Greetings again from the darkness. Based on th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sentiment == 1 means positive review, 0 for negatives.  \n",
    "\n",
    "The dataset is balanced:"
   ],
   "id": "744d0ad8815bf610"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:34:52.447607Z",
     "start_time": "2024-10-28T12:34:52.437779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Length of dataset: {df.shape[0]}')\n",
    "print(f'Number of positive and negative reviews: {df[df['sentiment'] == 1].shape[0]}, {df[df['sentiment'] == 0].shape[0]}')"
   ],
   "id": "db412ea250fa44df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 25000\n",
      "Number of positive and negative reviews: 12471, 12529\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Splitting Dataset: Train, Validation and Test subsets\n",
    "\n",
    "We will use 70% for training, 10% for validation and 20% for testing."
   ],
   "id": "46feaf2997a8c066"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:35:11.368997Z",
     "start_time": "2024-10-28T12:35:11.353188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=1310)\n",
    "valid_size = df.shape[0] * 0.1\n",
    "valid_frac_in_train_df = valid_size / train_df.shape[0]\n",
    "train_df, valid_df = train_test_split(train_df, test_size=valid_frac_in_train_df, random_state=1310)\n",
    "\n",
    "print(f'Train size: {train_df.shape[0]}')\n",
    "print(f'Valid size: {valid_df.shape[0]}')\n",
    "print(f'Test size: {test_df.shape[0]}')"
   ],
   "id": "e4b2707c9b2f9d80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 17500\n",
      "Valid size: 2500\n",
      "Test size: 5000\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenizing the dataset\n",
    "\n",
    "We will tokenize the texts into individual word tokens using the tokenizer provided by the pre-trained model class."
   ],
   "id": "d7b2359f7a3251fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:35:25.268415Z",
     "start_time": "2024-10-28T12:35:17.515005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_df['review'].values.tolist(), padding=True, truncation=True)\n",
    "valid_encodings = tokenizer(valid_df['review'].values.tolist(), padding=True, truncation=True)\n",
    "test_encodings = tokenizer(test_df['review'].values.tolist(), padding=True, truncation=True)"
   ],
   "id": "895fff2049b53658",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:35:47.167724Z",
     "start_time": "2024-10-28T12:35:47.162047Z"
    }
   },
   "cell_type": "code",
   "source": "train_encodings[0]",
   "id": "dbcc19ca25711f32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset class and DataLoader",
   "id": "7f469e2cf27ef376"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:35:50.366824Z",
     "start_time": "2024-10-28T12:35:50.359053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        super(IMDbDataset, self).__init__()\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # create a hashmap to hold the input tokens, attention masks and label\n",
    "        item = {key : torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.int32)\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ],
   "id": "298bdafe1dbbac01",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:35:52.973566Z",
     "start_time": "2024-10-28T12:35:52.968317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = IMDbDataset(train_encodings, train_df['sentiment'].values)\n",
    "valid_dataset = IMDbDataset(valid_encodings, valid_df['sentiment'].values)\n",
    "test_dataset = IMDbDataset(test_encodings, test_df['sentiment'].values)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "7792393a032c7c55",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning pre-trained BERT model\n",
    "### General settings"
   ],
   "id": "bedf2e945e73b80a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:35:56.328319Z",
     "start_time": "2024-10-28T12:35:55.749053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(1310) # for reproducibility\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_epochs = 3"
   ],
   "id": "a60fb14c73007688",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading the BERT model\n",
    "\n",
    "The downstream task we want to fine-tune the BERT model on is **sequence classification**.  \n",
    "\n",
    "`'distilbert-base-uncased'` is a streamlined, lightweight and uncased version of the BERT base model. It offers a smaller size while maintaining strong performance, making it more computationally efficient for tasks without sacrificing much accuracy."
   ],
   "id": "b92947aadd4372a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:36:04.749817Z",
     "start_time": "2024-10-28T12:35:59.048412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "bert_model.to(device)\n",
    "bert_model"
   ],
   "id": "c03e9377ebb59bb1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Defining accuracy function and optimizer",
   "id": "3466214d078eddd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:36:07.712561Z",
     "start_time": "2024-10-28T12:36:07.707891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy(model, dataloader, device):\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        \n",
    "        # compute accuracy by batches for RAM or VRAM limitations\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']\n",
    "            \n",
    "            pred_labels = torch.argmax(logits, 1)\n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (pred_labels == labels).sum()\n",
    "        \n",
    "        return correct_pred.float()/num_examples * 100"
   ],
   "id": "5fe5f211ff84a6a2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:36:17.106432Z",
     "start_time": "2024-10-28T12:36:11.918685Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = torch.optim.AdamW(bert_model.parameters(), lr=5e-5)",
   "id": "ba84ef9befebce4b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "f5db9b7e7ce66927"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T15:50:53.934432Z",
     "start_time": "2024-10-28T12:37:35.957734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # set for training\n",
    "    bert_model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # get data\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # reset gradients for each step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Forward\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask, labels=labels.long())\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "        \n",
    "        # 2. Backward and take step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Monitoring\n",
    "        if not batch_idx % 250:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs} .. '\n",
    "                  f'Batch {batch_idx}/{len(train_dataloader)} .. '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "            \n",
    "    # set for eval\n",
    "    bert_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print(f'Training accuracy: {accuracy(bert_model, train_dataloader, device):.2f}%\\n'\n",
    "              f'Validation accuracy: {accuracy(bert_model, valid_dataloader, device):.2f}%')\n",
    "        \n",
    "    print(f'Time taken: {(time.time() - start_time)/60:.2f} minutes')\n",
    "    \n",
    "print(f'Total time taken: {(time.time() - start_time)/60:.2f} minutes')"
   ],
   "id": "11c96ce9bc16bad3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 .. Batch 0/2188 .. Loss: 0.6782\n",
      "Epoch 1/3 .. Batch 250/2188 .. Loss: 0.2148\n",
      "Epoch 1/3 .. Batch 500/2188 .. Loss: 0.1224\n",
      "Epoch 1/3 .. Batch 750/2188 .. Loss: 0.6774\n",
      "Epoch 1/3 .. Batch 1000/2188 .. Loss: 0.1526\n",
      "Epoch 1/3 .. Batch 1250/2188 .. Loss: 0.2069\n",
      "Epoch 1/3 .. Batch 1500/2188 .. Loss: 0.1844\n",
      "Epoch 1/3 .. Batch 1750/2188 .. Loss: 0.4709\n",
      "Epoch 1/3 .. Batch 2000/2188 .. Loss: 0.0547\n",
      "Training accuracy: 96.23%\n",
      "Validation accuracy: 91.60%\n",
      "Time taken: 63.75 minutes\n",
      "Epoch 2/3 .. Batch 0/2188 .. Loss: 0.0077\n",
      "Epoch 2/3 .. Batch 250/2188 .. Loss: 0.0832\n",
      "Epoch 2/3 .. Batch 500/2188 .. Loss: 0.0517\n",
      "Epoch 2/3 .. Batch 750/2188 .. Loss: 0.0131\n",
      "Epoch 2/3 .. Batch 1000/2188 .. Loss: 0.1785\n",
      "Epoch 2/3 .. Batch 1250/2188 .. Loss: 0.0117\n",
      "Epoch 2/3 .. Batch 1500/2188 .. Loss: 0.0265\n",
      "Epoch 2/3 .. Batch 1750/2188 .. Loss: 0.0650\n",
      "Epoch 2/3 .. Batch 2000/2188 .. Loss: 0.0398\n",
      "Training accuracy: 97.47%\n",
      "Validation accuracy: 90.52%\n",
      "Time taken: 128.29 minutes\n",
      "Epoch 3/3 .. Batch 0/2188 .. Loss: 0.1813\n",
      "Epoch 3/3 .. Batch 250/2188 .. Loss: 0.1695\n",
      "Epoch 3/3 .. Batch 500/2188 .. Loss: 0.0094\n",
      "Epoch 3/3 .. Batch 750/2188 .. Loss: 0.0073\n",
      "Epoch 3/3 .. Batch 1000/2188 .. Loss: 0.0083\n",
      "Epoch 3/3 .. Batch 1250/2188 .. Loss: 0.0897\n",
      "Epoch 3/3 .. Batch 1500/2188 .. Loss: 0.2483\n",
      "Epoch 3/3 .. Batch 1750/2188 .. Loss: 0.0429\n",
      "Epoch 3/3 .. Batch 2000/2188 .. Loss: 0.0592\n",
      "Training accuracy: 99.58%\n",
      "Validation accuracy: 92.44%\n",
      "Time taken: 193.30 minutes\n",
      "Total time taken: 193.30 minutes\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T15:57:05.195852Z",
     "start_time": "2024-10-28T15:57:04.600034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_save_path = \"./finetuned_imdb_bert\"\n",
    "bert_model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f'Model and tokenizer saved to {model_save_path}')"
   ],
   "id": "dcaa16a4d8811a8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./finetuned_imdb_bert\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluating",
   "id": "56ef8ac536cbb98f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T16:02:52.212489Z",
     "start_time": "2024-10-28T15:57:47.504831Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 91.28%\n"
     ]
    }
   ],
   "execution_count": 17,
   "source": "print(f'Test accuracy: {accuracy(bert_model, test_dataloader, device):.2f}%')",
   "id": "fbc7a1585addc27c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The fine-tuned DistilBERT model achieved a validation accuracy of 93% and a test accuracy of 91%, demonstrating strong sentiment classification performance. While accuracy could likely improve with training on the full dataset, this approach prioritized efficiency, as training on the complete data would have been more computationally expensive.",
   "id": "ac5e372b6dd7c579"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "50a23759275cb55c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
